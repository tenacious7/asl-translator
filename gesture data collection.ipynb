{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b75783-257a-4e61-aff3-26511ef4a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install mediapipe\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "IMAGE_PATH = \"gesture_images\"\n",
    "GESTURES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'OPEN_HAND']\n",
    "CURRENT_GESTURE = 0\n",
    "IMG_COUNT = 0\n",
    "IMG_TARGET = 400  \n",
    "ROI_BOX = (50, 100, 350, 400) \n",
    "minValue = 70  \n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for gesture in GESTURES:\n",
    "    gesture_dir = os.path.join(IMAGE_PATH, gesture)\n",
    "    if not os.path.exists(gesture_dir):\n",
    "        os.makedirs(gesture_dir)\n",
    "\n",
    "def process_frame(frame, process_landmarks=True):\n",
    "    global IMG_COUNT, data, labels\n",
    "\n",
    "    x1, y1, x2, y2 = ROI_BOX\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blurred_roi = cv2.GaussianBlur(gray_roi, (5, 5), 2)\n",
    "    \n",
    "    th3 = cv2.adaptiveThreshold(blurred_roi, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    _, processed_roi = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    if process_landmarks and IMG_COUNT < IMG_TARGET:\n",
    "        rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_roi)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                # Check if hand is fully within ROI\n",
    "                all_inside = all(0 <= lm.x <= 1 and 0 <= lm.y <= 1 for lm in hand_landmarks.landmark)\n",
    "                \n",
    "                if all_inside:\n",
    "                    # Save landmarks and corresponding gesture label\n",
    "                    landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "                    data.append(landmarks)\n",
    "                    labels.append(GESTURES[CURRENT_GESTURE])\n",
    "\n",
    "                    # Create a copy of processed_roi for drawing\n",
    "                    landmark_image = cv2.cvtColor(processed_roi, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                    # Draw landmarks on the landmark_image\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        landmark_image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                    # Save the processed image with landmarks in its gesture folder\n",
    "                    gesture_folder = os.path.join(IMAGE_PATH, GESTURES[CURRENT_GESTURE])\n",
    "                    img_filename = os.path.join(gesture_folder, f\"img_{IMG_COUNT:04d}.png\")\n",
    "                    cv2.imwrite(img_filename, landmark_image)\n",
    "\n",
    "                    # Update image count\n",
    "                    IMG_COUNT += 1\n",
    "\n",
    "                    # Draw landmarks on the original ROI for display\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        roi,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    return processed_roi, roi\n",
    "\n",
    "# Main video capture loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set a specific size for the window\n",
    "window_width = 800\n",
    "window_height = 600\n",
    "cv2.namedWindow(\"Data Collection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Data Collection\", window_width, window_height)\n",
    "\n",
    "last_process_time = time.time()\n",
    "process_interval = 0.1  # Process landmarks every 100ms\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_process_time >= process_interval:\n",
    "        # Process the frame and get the ROI with landmarks\n",
    "        processed_roi, roi_with_landmarks = process_frame(frame, process_landmarks=True)\n",
    "        last_process_time = current_time\n",
    "    else:\n",
    "        # Just get the ROI without processing landmarks\n",
    "        processed_roi, roi_with_landmarks = process_frame(frame, process_landmarks=False)\n",
    "\n",
    "    # Draw the fixed ROI box on the frame\n",
    "    x1, y1, x2, y2 = ROI_BOX\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the ROI with landmarks in the frame\n",
    "    frame[y1:y2, x1:x2] = roi_with_landmarks\n",
    "\n",
    "    # Display current gesture and progress\n",
    "    cv2.putText(frame, f\"Capturing Gesture: {GESTURES[CURRENT_GESTURE]}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Images Captured: {IMG_COUNT}/{IMG_TARGET}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Check if target images have been captured\n",
    "    if IMG_COUNT >= IMG_TARGET:\n",
    "        cv2.putText(frame, \"Gesture capture complete. Press 'n' for the next gesture.\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "    # Key controls for gesture collection\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # ESC to quit and save data/labels\n",
    "    if key == 27:  # ESC key to quit\n",
    "        break\n",
    "    \n",
    "    # When 'N' key is pressed and target images have been captured\n",
    "    elif key == ord('n') and IMG_COUNT >= IMG_TARGET:\n",
    "        # Move to the next gesture\n",
    "        CURRENT_GESTURE = (CURRENT_GESTURE + 1) % len(GESTURES)\n",
    "        IMG_COUNT = 0  # Reset image count for the new gesture\n",
    "        print(f\"Gesture {GESTURES[CURRENT_GESTURE]} is ready. Capturing will start now.\")\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save collected data and labels to files\n",
    "np.save('hand_landmarks.npy', np.array(data))\n",
    "np.save('labels.npy', np.array(labels))\n",
    "print(\"Data and labels saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267d120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
