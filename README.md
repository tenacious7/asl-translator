#  ASL Gesture Recognition AI âœ‹ğŸ”¤

> **Empowering Communication Through AI | Helping the Deaf and Mute Community**

---

## ğŸ§  Overview

**HOPE** is an AI-based system designed to recognize hand gestures from the **American Sign Language (ASL)** and convert them into **text or speech** in real-time. Using machine learning and computer vision, it bridges the communication gap between the deaf/mute community and the hearing world.

---

## ğŸŒŸ Key Features

- âœ‹ **ASL Hand Gesture Detection**
- ğŸ§  **Deep Learning-Based Recognition (LSTM/MediaPipe)**
- ğŸ—£ï¸ **Text and Voice Output**
- ğŸ’¡ **Real-Time Prediction Interface**
- ğŸ–¥ï¸ **Web-based UI using Flask**

---

## ğŸ”§ Tech Stack

| Component       | Technology Used         |
|----------------|--------------------------|
| Language        | Python                   |
| ML Framework    | TensorFlow / Keras       |
| Vision Model    | MediaPipe Hand Tracking  |
| UI Framework    | Flask, HTML/CSS          |
| Others          | Label Encoder, NumPy     |

---

## ğŸ¯ Objective

> To assist individuals with hearing or speaking disabilities by enabling computers to **understand ASL hand gestures** and translate them into a **readable or audible format** using AI.

---

ğŸ“¸ Visual Showcase of ASL Project
ğŸš€ 1. Flask Landing Page
<p align="center"> <img src="asl_landing_page .png" alt="Flask Landing Page" width="600"/> </p> <p align="center"> <b>Figure 1:</b> Flask Web Application â€“ Home Page for ASL Gesture Recognition </p>
ğŸ§ª 2. Real-time Gesture Testing
<p align="center"> <img src="assets/Testing image.png" alt="ASL Model Testing" width="600"/> </p> <p align="center"> <b>Figure 2:</b> Real-Time Gesture Detection and Prediction Output from ASL Model </p>
ğŸ”„ 3. Flask Integration with ASL Model
<p align="center"> <img src="assets/flask_app.png" alt="ASL Flask Integration" width="600"/> </p> <p align="center"> <b>Figure 3:</b> Flask Backend Integrated with AI Model for Live ASL Interpretation </p>

---

## ğŸš€ How It Works

1. **User shows a hand sign** in front of the webcam.
2. **MediaPipe** detects and extracts hand landmarks.
3. **Trained AI model (LSTM)** classifies the gesture.
4. **Output is displayed** on-screen as text and/or converted to speech.

---

## ğŸ“¦ Project Structure

project-root/
â”‚
â”œâ”€â”€ model/ # Trained model (hope.keras)
â”œâ”€â”€ static/ # CSS, JS, and images
â”œâ”€â”€ templates/ # HTML files
â”œâ”€â”€ app.py # Flask backend
â”œâ”€â”€ utils/ # Helper functions
â”œâ”€â”€ label_encoder.pkl # Encoded ASL labels
â”œâ”€â”€ README.md # This file
â””â”€â”€ requirements.txt # Python dependencies

---

## ğŸ“ˆ Future Enhancements

- âœ¨ Add full **sentence prediction**
- ğŸŒ Support **multiple sign languages**
- ğŸ“± Build a mobile app version
- ğŸ§ª Improve model accuracy with more data

---

## ğŸ™Œ Acknowledgments

- American Sign Language community
- TensorFlow & MediaPipe developers
- OpenCV & Python ML ecosystem

---

## ğŸ¤ Letâ€™s Connect!

> Created by **Brijesh Kumar** â€“ Passionate about AI & Accessibility  
> Feel free to fork, star â­ the project, and contribute!
